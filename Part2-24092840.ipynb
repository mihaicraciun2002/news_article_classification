{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf9dbac-14f9-4912-938c-489d0ce9a1c0",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204b3e78-ae32-4afa-8fb1-c5ba667b7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_raw = pd.read_csv('bbc-text.csv', encoding = 'unicode_escape')\n",
    "\n",
    "# Process the raw dataset into an array of instances\n",
    "dataset_full = []\n",
    "\n",
    "for index in range(len(dataset_raw)):\n",
    "    label = dataset_raw[\"category\"][index]\n",
    "    dataset_full.append((dataset_raw[\"text\"][index], label))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46c974-708c-47c3-8387-85507ce0f75e",
   "metadata": {},
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4b434-e782-4d2b-90b3-f5ee225e8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function to split the dataset\n",
    "# input: dataset (array of instances)\n",
    "# output: train, dev, test sets (array of instances)\n",
    "def get_split_dataset(dataset):\n",
    "    index_list = []\n",
    "    train_set = []\n",
    "    dev_set = []\n",
    "    test_set = []\n",
    "    # Compute the sizes of the sets\n",
    "    # 80% for the training set, 10% for the dev set, 10% for the test set\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(dataset_size * 0.8)\n",
    "    dev_size = int(dataset_size * 0.1)\n",
    "    test_size = dataset_size - train_size - dev_size\n",
    "    \n",
    "    # Create a list of indexes\n",
    "    # 0 corresponds to the training set\n",
    "    # 1 corresponds to the dev set\n",
    "    # 2 corresponds to the test set\n",
    "    for index in range(dataset_size):\n",
    "        if index < train_size:\n",
    "            index_list.append(0)\n",
    "        elif index < train_size + dev_size:\n",
    "            index_list.append(1)\n",
    "        elif index < dataset_size:\n",
    "            index_list.append(2)\n",
    "\n",
    "    # Randomize the index list, and use it to create the\n",
    "    # three sets\n",
    "    random.shuffle(index_list)\n",
    "    \n",
    "    for index in range(dataset_size):\n",
    "        if index_list[index] == 0:\n",
    "            train_set.append(dataset[index])\n",
    "        elif index_list[index] == 1:\n",
    "            dev_set.append(dataset[index])\n",
    "        elif index_list[index] == 2:\n",
    "            test_set.append(dataset[index])\n",
    "\n",
    "    # Randomize the sets once again\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(dev_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    return train_set, dev_set, test_set\n",
    "\n",
    "# Randomize the order of the dataset\n",
    "random.shuffle(dataset_full)\n",
    "train_set, dev_set, test_set = get_split_dataset(dataset_full)\n",
    "\n",
    "print(len(dataset_full))\n",
    "print(len(train_set))\n",
    "print(len(dev_set))\n",
    "print(len(test_set))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8928f97-de9d-40ca-a765-de6bf5ce8404",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343d36e-db08-45de-b884-5e10709b0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import operator\n",
    "import requests\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"``\")\n",
    "\n",
    "# Function taken from Session 1 of the Practicals\n",
    "# Transforms a document into an array of tokens\n",
    "# Input: document\n",
    "# Output: list of tokens\n",
    "def get_list_tokens(document):\n",
    "    # Split the document into sentences\n",
    "    sentence_split = nltk.tokenize.sent_tokenize(document)\n",
    "    list_tokens=[]\n",
    "    for sentence in sentence_split:\n",
    "        # Split each sentence into words\n",
    "        list_tokens_sentence = nltk.tokenize.word_tokenize(sentence)\n",
    "        # Make all the words lowercase and lemmatize them\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens\n",
    "\n",
    "# Function slightly modified from Session 2 of the Practicals\n",
    "# Process a dataset's inputs\n",
    "# Input: training_set (array of instances), num_features (how many features should be kept)\n",
    "# Output: array of instances, with each input being pre-processed (lemmatized, lowercased, stopwords taken out)\n",
    "def pre_process(dataset):\n",
    "    processed_dataset = []\n",
    "    for instance in dataset:\n",
    "        # Tokenize the input\n",
    "        sentence_tokens = get_list_tokens(instance[0])\n",
    "        processed_document = \"\"\n",
    "        # Re-concatenate the tokens\n",
    "        for word in sentence_tokens:\n",
    "            if word in stopwords: continue\n",
    "            processed_document += word + \" \"\n",
    "\n",
    "        # Cut the last \" \" added\n",
    "        processed_document = processed_document[:-1]\n",
    "\n",
    "        # Create the processed dataset\n",
    "        processed_dataset.append([processed_document, instance[1]])\n",
    "\n",
    "    return processed_dataset\n",
    "\n",
    "# Similar to pre_process, but for a single input instead of a dataset\n",
    "# Input: data input in its original form\n",
    "# Output: pre-processed input\n",
    "def process_doc(instance_X):\n",
    "    sentence_tokens = get_list_tokens(instance_X)\n",
    "    processed_document = \"\"\n",
    "    for word in sentence_tokens:\n",
    "        if word in stopwords: continue\n",
    "        processed_document += word + \" \"\n",
    "    processed_document = processed_document[:-1]\n",
    "    return processed_document\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5014c2-ae81-4db8-bf54-cc9079895109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the training set\n",
    "processed_train_set = pre_process(train_set)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcd385-fb16-4261-95f2-11b151ffd2c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See the first instance\n",
    "print(train_set[:1])\n",
    "print(processed_train_set[:1])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addff178-a5f5-46f7-bf81-557d31104dc7",
   "metadata": {},
   "source": [
    "## Tf-Idf vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdf0af-1396-48f7-8a26-3d08c966ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Obtain a tf-idf vectorizer\n",
    "# Input: max_features (maximum number of features for this vectorizer), training set\n",
    "# Output: tf-idf vectorizer\n",
    "def get_tfidf_vectorizer(max_features, train_set):\n",
    "    vectorizer = TfidfVectorizer(max_features = max_features)\n",
    "    X_train = []\n",
    "\n",
    "    for instance in train_set:\n",
    "        X_train.append(instance[0])\n",
    "    \n",
    "    vectorizer.fit(X_train)\n",
    "\n",
    "    return vectorizer\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d83671-a38b-4a89-abf0-828ec7ef952d",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f1d03-3cdf-4eb1-95c2-5d42c4d1feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "W2VEC_NUM_FEATURES = 300\n",
    "\n",
    "# Outputs the w2vec model trained on Google News\n",
    "def get_w2vec_vectorizer():\n",
    "    return api.load('word2vec-google-news-300')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e71fbb-f8cd-4987-89ed-dc7a5760e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec = get_w2vec_vectorizer()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9499c-7d8c-4cba-9eff-6d8a6205af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the dimensions of w2vec embeddings are correct\n",
    "print(len(w2vec['movie']))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1dea1",
   "metadata": {},
   "source": [
    "## Pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8af1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos tags for the nltk tagger with the universal tagset\n",
    "pos_tags = [\n",
    "    \"ADJ\",\n",
    "    \"ADP\",\n",
    "    \"ADV\",\n",
    "    \"CONJ\",\n",
    "    \"DET\",\n",
    "    \"NOUN\",\n",
    "    \"NUM\",\n",
    "    \"PRT\",\n",
    "    \"PRON\",\n",
    "    \"VERB\",\n",
    "    \".\",\n",
    "    \"X\"\n",
    "]\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "def get_pos_tagger():\n",
    "    return nltk.pos_tag\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an instance of the pos tagger\n",
    "pos_tagger = get_pos_tagger()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6024d138-7226-41e3-b0ed-7571852c3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an input, process it into a vector\n",
    "# Input: document input, tfidf vectorizer, pos_tagger\n",
    "def get_vector(instance_X, tfidf_vectorizer, pos_tagger):\n",
    "    global W2VEC_NUM_FEATURES\n",
    "    global pos_tags\n",
    "    # get the pos_tag vector first, before pre-processing the input\n",
    "    tags = pos_tagger(nltk.tokenize.word_tokenize(instance_X), tagset = 'universal', lang = 'eng')\n",
    "\n",
    "    pos_tag_vec = np.zeros(len(pos_tags))\n",
    "    total_pos_tags = 0\n",
    "\n",
    "    # Create the pos_tag vector by keeping counts of the tags in the document\n",
    "    for tag_index in range(len(pos_tag_vec)):\n",
    "        tag = pos_tags[tag_index]\n",
    "        for index in range(len(tags)):\n",
    "            (word, word_tag) = tags[index]\n",
    "            if word_tag == tag:\n",
    "                pos_tag_vec[tag_index] += 1\n",
    "                total_pos_tags += 1\n",
    "\n",
    "    # Normalize the tag counts\n",
    "    for tag_index in range(len(pos_tag_vec)):\n",
    "        pos_tag_vec[tag_index] = pos_tag_vec[tag_index] / total_pos_tags\n",
    "\n",
    "    # print(pos_tag_vec)\n",
    "\n",
    "    # First, pre-process the input\n",
    "    processed_instance = process_doc(instance_X)\n",
    "    tf_vec = tfidf_vectorizer.transform([processed_instance]).todense().A1\n",
    "    word_list = nltk.tokenize.word_tokenize(processed_instance)\n",
    "    w2_vec = np.zeros(W2VEC_NUM_FEATURES)\n",
    "    # Average the w2vec vectors\n",
    "    for word in word_list:\n",
    "        word_vec = []\n",
    "        if word in w2vec.key_to_index:\n",
    "            word_vec = w2vec[word]\n",
    "        else:\n",
    "            word_vec = np.zeros(W2VEC_NUM_FEATURES)\n",
    "        for index in range(W2VEC_NUM_FEATURES):\n",
    "            w2_vec[index] = w2_vec[index] + word_vec[index]\n",
    "    for index in range(W2VEC_NUM_FEATURES):\n",
    "        w2_vec[index] = w2_vec[index] / len(word_list)\n",
    "    \n",
    "    # Combine the 3 representations\n",
    "    combined_vec = np.concatenate([tf_vec, w2_vec, pos_tag_vec])\n",
    "    return combined_vec\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75321aad-3f45-4f8b-8805-865275073f10",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be79f39-374c-4618-b69f-cd7dc674ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = get_tfidf_vectorizer(1000, processed_train_set)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b2b58-c359-4474-9ac8-85f2c5fde5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "X_vec_train = []\n",
    "Y_train = []\n",
    "for instance in train_set:\n",
    "    X_vec_train.append(get_vector(instance[0], tfidf_vec, pos_tagger))\n",
    "    Y_train.append(instance[1])\n",
    "\n",
    "print(len(X_vec_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d5b9d-5c4a-4b06-b41e-88d07408e0d3",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b61d38-6d87-4db9-ace3-a575fe4aa55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output: Feature selector that keeps num_features\n",
    "def feature_selection(num_features):\n",
    "    return SelectKBest(k = num_features).fit(X_vec_train, Y_train)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d5181-cb05-41fc-aa85-7dd8dc594d9a",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee22164-17c7-4d4a-a6df-8699e06aae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dev and test sets\n",
    "X_vec_test = []\n",
    "Y_test = []\n",
    "\n",
    "X_vec_dev = []\n",
    "Y_dev = []\n",
    "\n",
    "for instance in test_set:\n",
    "    Y_test.append(instance[1])\n",
    "    X_vec_test.append(get_vector(instance[0], tfidf_vec, pos_tagger))\n",
    "\n",
    "for instance in dev_set:\n",
    "    Y_dev.append(instance[1])\n",
    "    X_vec_dev.append(get_vector(instance[0], tfidf_vec, pos_tagger))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044e07d-385b-4397-9058-96e9a6bebef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "list_num_features = [300, 500, 750, 1000]\n",
    "\n",
    "log_reg_clf_bst = 0\n",
    "accuracy_score_bst = 0\n",
    "feature_selector_bst = 0\n",
    "num_features_bst = 0\n",
    "\n",
    "# Train and tune the model\n",
    "\n",
    "for num_features in list_num_features:\n",
    "    feature_selector = feature_selection(num_features)\n",
    "    X_train_new = feature_selector.transform(X_vec_train)\n",
    "    log_reg_clf = SGDClassifier(loss = 'log_loss').fit(X_train_new, Y_train)\n",
    "    \n",
    "    X_dev_new = feature_selector.transform(X_vec_dev)\n",
    "    Y_dev_pred = log_reg_clf.predict(X_dev_new)\n",
    "    accuracy = accuracy_score(Y_dev, Y_dev_pred)\n",
    "\n",
    "    if accuracy > accuracy_score_bst:\n",
    "        log_reg_clf_bst = log_reg_clf\n",
    "        accuracy_score_bst = accuracy\n",
    "        feature_selector_bst = feature_selector\n",
    "        num_features_bst = num_features\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194141da-6ac9-4516-b159-b6b9fabe491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use the tuned model to test\n",
    "X_test_new = feature_selector_bst.transform(X_vec_test)\n",
    "\n",
    "Y_test_pred = log_reg_clf_bst.predict(X_test_new)\n",
    "\n",
    "\n",
    "print(num_features_bst)\n",
    "\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
