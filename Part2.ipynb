{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf9dbac-14f9-4912-938c-489d0ce9a1c0",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "204b3e78-ae32-4afa-8fb1-c5ba667b7633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset_raw = pd.read_csv('bbc-text.csv', encoding = 'unicode_escape')\n",
    "\n",
    "# Process the raw dataset into an array of instances\n",
    "dataset_full = []\n",
    "\n",
    "# Original label to numerical label conversion\n",
    "label_to_index = {\n",
    "    \"business\" : 0,\n",
    "    \"sport\" : 1,\n",
    "    \"tech\" : 2,\n",
    "    \"politics\" : 3,\n",
    "    \"entertainment\" : 4}\n",
    "\n",
    "for index in range(len(dataset_raw)):\n",
    "    label = label_to_index[dataset_raw[\"category\"][index]]\n",
    "    dataset_full.append((dataset_raw[\"text\"][index], label))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46c974-708c-47c3-8387-85507ce0f75e",
   "metadata": {},
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3ed4b434-e782-4d2b-90b3-f5ee225e8bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n",
      "1780\n",
      "222\n",
      "223\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Function to split the dataset\n",
    "# input: dataset (array of instances)\n",
    "# output: train, dev, test sets (array of instances)\n",
    "def get_split_dataset(dataset):\n",
    "    index_list = []\n",
    "    train_set = []\n",
    "    dev_set = []\n",
    "    test_set = []\n",
    "    # Compute the sizes of the sets\n",
    "    # 80% for the training set, 10% for the dev set, 10% for the test set\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(dataset_size * 0.8)\n",
    "    dev_size = int(dataset_size * 0.1)\n",
    "    test_size = dataset_size - train_size - dev_size\n",
    "    \n",
    "    # Create a list of indexes\n",
    "    # 0 corresponds to the training set\n",
    "    # 1 corresponds to the dev set\n",
    "    # 2 corresponds to the test set\n",
    "    for index in range(dataset_size):\n",
    "        if index < train_size:\n",
    "            index_list.append(0)\n",
    "        elif index < train_size + dev_size:\n",
    "            index_list.append(1)\n",
    "        elif index < dataset_size:\n",
    "            index_list.append(2)\n",
    "\n",
    "    # Randomize the index list, and use it to create the\n",
    "    # three sets\n",
    "    random.shuffle(index_list)\n",
    "    \n",
    "    for index in range(dataset_size):\n",
    "        if index_list[index] == 0:\n",
    "            train_set.append(dataset[index])\n",
    "        elif index_list[index] == 1:\n",
    "            dev_set.append(dataset[index])\n",
    "        elif index_list[index] == 2:\n",
    "            test_set.append(dataset[index])\n",
    "\n",
    "    # Randomize the sets once again\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(dev_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    return train_set, dev_set, test_set\n",
    "\n",
    "# Randomize the order of the dataset\n",
    "random.shuffle(dataset_full)\n",
    "train_set, dev_set, test_set = get_split_dataset(dataset_full)\n",
    "\n",
    "print(len(dataset_full))\n",
    "print(len(train_set))\n",
    "print(len(dev_set))\n",
    "print(len(test_set))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8928f97-de9d-40ca-a765-de6bf5ce8404",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6343d36e-db08-45de-b884-5e10709b0ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import operator\n",
    "import requests\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"``\")\n",
    "\n",
    "# Function taken from Session 1 of the Practicals\n",
    "# Transforms a document into an array of tokens\n",
    "# Input: document\n",
    "# Output: list of tokens\n",
    "def get_list_tokens(document):\n",
    "    # Split the document into sentences\n",
    "    sentence_split = nltk.tokenize.sent_tokenize(document)\n",
    "    list_tokens=[]\n",
    "    for sentence in sentence_split:\n",
    "        # Split each sentence into words\n",
    "        list_tokens_sentence = nltk.tokenize.word_tokenize(sentence)\n",
    "        # Make all the words lowercase and lemmatize them\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens\n",
    "\n",
    "# Function slightly modified from Session 2 of the Practicals\n",
    "# Process a dataset's inputs\n",
    "# Input: training_set (array of instances), num_features (how many features should be kept)\n",
    "# Output: array of instances, with each input being pre-processed (lemmatized, lowercased, stopwords taken out)\n",
    "def pre_process(dataset):\n",
    "    processed_dataset = []\n",
    "    for instance in dataset:\n",
    "        # Tokenize the input\n",
    "        sentence_tokens = get_list_tokens(instance[0])\n",
    "        processed_document = \"\"\n",
    "        # Re-concatenate the tokens\n",
    "        for word in sentence_tokens:\n",
    "            if word in stopwords: continue\n",
    "            processed_document += word + \" \"\n",
    "\n",
    "        # Cut the last \" \" added\n",
    "        processed_document = processed_document[:-1]\n",
    "\n",
    "        # Create the processed dataset\n",
    "        processed_dataset.append([processed_document, instance[1]])\n",
    "\n",
    "    return processed_dataset\n",
    "\n",
    "# Similar to pre_process, but for a single input instead of a dataset\n",
    "# Input: data input in its original form\n",
    "# Output: pre-processed input\n",
    "def process_doc(instance_X):\n",
    "    sentence_tokens = get_list_tokens(instance_X)\n",
    "    processed_document = \"\"\n",
    "    for word in sentence_tokens:\n",
    "        if word in stopwords: continue\n",
    "        processed_document += word + \" \"\n",
    "    processed_document = processed_document[:-1]\n",
    "    return processed_document\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7f5014c2-ae81-4db8-bf54-cc9079895109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Pre-process the training set\n",
    "processed_train_set = pre_process(train_set)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "cfdcd385-fb16-4261-95f2-11b151ffd2c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('open source leaders slam patents the war of words between microsoft and the open source movement heated up this week as linux founder linus torvalds led an attack on software patents.  in a panel discussion at a linux summit in california mr torvalds said software patents were a problem for the open source movement. mitchell kapor  chairman of the mozilla foundation  warned that microsoft could use patent lawsuits in the future. linux is a freely-available alternative to microsoft s windows. it relies on a community of programmers for its development and is based on open source principles  which allow others to use and modify it without having to pay licence fees. the attack on software patents comes at a time when ibm has made 500 of its patents freely available. other companies are expected to follow suit.  there are between 150 000 and 300 000 registered software patents in the us and open source developers argue that many should never have been granted. this is a view corroborated by the uk patent office.  some of the patents have dubious validity and are being wielded by some big companies to force smaller companies to buy licenses in the knowledge that they can t afford to take them to court   said dr jeremy philpott of the uk patent office. some panel members are worried that microsoft would issue a series of patent lawsuits in the future.  if totally pushed to the wall - because their business model no longer holds up in an era in which open source is an economically superior way to produce software...of course they re going to unleash the wmds   mr kapor is reported as saying. microsoft did not want to comment directly  referring the issue instead to trade body intellect  of which it is a member.  as far as intellect is concerned  open source and patents have co-existed for many years without problems   said spokeswoman jill sutherland.  the industry respects the open source movement and in fact many of the members we represent use the open source system to develop software    we think the important point to make is that companies should be able to choose between patents  copyrights and open source as to the treatment of their intellectual discoveries  and not be forced into using one or the other   she added.', 2)]\n",
      "[['open source leader slam patent war word microsoft open source movement heated week linux founder linus torvalds led attack software patent panel discussion linux summit california mr torvalds said software patent problem open source movement mitchell kapor chairman mozilla foundation warned microsoft could use patent lawsuit future linux freely-available alternative microsoft window relies community programmer development based open source principle allow others use modify without pay licence fee attack software patent come time ibm ha made 500 patent freely available company expected follow suit 150 000 300 000 registered software patent u open source developer argue many never granted view corroborated uk patent office patent dubious validity wielded big company force smaller company buy license knowledge afford take court said dr jeremy philpott uk patent office panel member worried microsoft would issue series patent lawsuit future totally pushed wall - business model longer hold era open source economically superior way produce software ... course going unleash wmd mr kapor reported saying microsoft want comment directly referring issue instead trade body intellect member far intellect concerned open source patent co-existed many year without problem said spokeswoman jill sutherland industry respect open source movement fact many member represent use open source system develop software think important point make company able choose patent copyright open source treatment intellectual discovery forced using one added', 2]]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# See the first instance\n",
    "print(train_set[:1])\n",
    "print(processed_train_set[:1])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addff178-a5f5-46f7-bf81-557d31104dc7",
   "metadata": {},
   "source": [
    "## Tf-Idf vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d9bdf0af-1396-48f7-8a26-3d08c966ef95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Obtain a tf-idf vectorizer\n",
    "# Input: max_features (maximum number of features for this vectorizer), training set\n",
    "# Output: tf-idf vectorizer\n",
    "def get_tfidf_vectorizer(max_features, train_set):\n",
    "    vectorizer = TfidfVectorizer(max_features = max_features)\n",
    "    X_train = []\n",
    "\n",
    "    for instance in train_set:\n",
    "        X_train.append(instance[0])\n",
    "    \n",
    "    vectorizer.fit(X_train)\n",
    "\n",
    "    return vectorizer\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d83671-a38b-4a89-abf0-828ec7ef952d",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b45f1d03-3cdf-4eb1-95c2-5d42c4d1feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "W2VEC_NUM_FEATURES = 300\n",
    "\n",
    "# Outputs the w2vec model trained on Google News\n",
    "def get_w2vec_vectorizer():\n",
    "    return api.load('word2vec-google-news-300')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "72e71fbb-f8cd-4987-89ed-dc7a5760e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------------] 1.4% 23.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-------------------------------------------------] 3.5% 59.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==------------------------------------------------] 5.9% 97.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====----------------------------------------------] 8.2% 135.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====---------------------------------------------] 10.1% 168.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======--------------------------------------------] 12.4% 205.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======-------------------------------------------] 14.6% 242.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========----------------------------------------] 21.9% 363.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========---------------------------------------] 23.5% 391.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============-------------------------------------] 27.8% 462.0/1662.8MB downloaded\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "w2vec = get_w2vec_vectorizer()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7dc9499c-7d8c-4cba-9eff-6d8a6205af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Make sure the dimensions of w2vec embeddings are correct\n",
    "print(len(w2vec['movie']))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6024d138-7226-41e3-b0ed-7571852c3127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Given an input, process it into a vector\n",
    "# Input: document input, tfidf vectorizer\n",
    "def get_vector(instance_X, tfidf_vectorizer):\n",
    "    global W2VEC_NUM_FEATURES\n",
    "    # First, pre-process the input\n",
    "    processed_instance = process_doc(instance_X)\n",
    "    # \n",
    "    tf_vec = tfidf_vectorizer.transform([processed_instance]).todense().A1\n",
    "    word_list = nltk.tokenize.word_tokenize(processed_instance)\n",
    "    w2_vec = np.zeros(W2VEC_NUM_FEATURES)\n",
    "    for word in word_list:\n",
    "        word_vec = []\n",
    "        if word in w2vec.key_to_index:\n",
    "            word_vec = w2vec[word]\n",
    "        else:\n",
    "            word_vec = np.zeros(W2VEC_NUM_FEATURES)\n",
    "        for index in range(W2VEC_NUM_FEATURES):\n",
    "            w2_vec[index] = w2_vec[index] + word_vec[index]\n",
    "    for index in range(W2VEC_NUM_FEATURES):\n",
    "        w2_vec[index] = w2_vec[index] / len(word_list)\n",
    "    \n",
    "    combined_vec = np.concatenate([tf_vec, w2_vec])\n",
    "    return combined_vec\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75321aad-3f45-4f8b-8805-865275073f10",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5be79f39-374c-4618-b69f-cd7dc674ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = get_tfidf_vectorizer(1000, processed_train_set)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "509b2b58-c359-4474-9ac8-85f2c5fde5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07052182  0.          0.         ... -0.03545366 -0.00965458\n",
      "  0.0273459 ]\n",
      "1300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "X_vec_train = []\n",
    "Y_train = []\n",
    "for instance in train_set:\n",
    "    X_vec_train.append(get_vector(instance[0], tfidf_vec))\n",
    "    Y_train.append(instance[1])\n",
    "\n",
    "print(X_vec_train[0])\n",
    "print(len(X_vec_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d5b9d-5c4a-4b06-b41e-88d07408e0d3",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f8b61d38-6d87-4db9-ace3-a575fe4aa55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Output: Feature selector that keeps num_features\n",
    "def feature_selection(num_features):\n",
    "    return SelectKBest(k = num_features).fit(X_vec_train, Y_train)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d5181-cb05-41fc-aa85-7dd8dc594d9a",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7ee22164-17c7-4d4a-a6df-8699e06aae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Process the dev and test sets\n",
    "X_vec_test = []\n",
    "Y_test = []\n",
    "\n",
    "X_vec_dev = []\n",
    "Y_dev = []\n",
    "\n",
    "for instance in test_set:\n",
    "    Y_test.append(instance[1])\n",
    "    X_vec_test.append(get_vector(instance[0], tfidf_vec))\n",
    "\n",
    "for instance in dev_set:\n",
    "    Y_dev.append(instance[1])\n",
    "    X_vec_dev.append(get_vector(instance[0], tfidf_vec))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4044e07d-385b-4397-9058-96e9a6bebef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "list_num_features = [300, 500, 750, 1000]\n",
    "\n",
    "log_reg_clf_bst = 0\n",
    "accuracy_score_bst = 0\n",
    "feature_selector_bst = 0\n",
    "num_features_bst = 0\n",
    "\n",
    "# Train and tune the model\n",
    "\n",
    "for num_features in list_num_features:\n",
    "    feature_selector = feature_selection(num_features)\n",
    "    X_train_new = feature_selector.transform(X_vec_train)\n",
    "    log_reg_clf = SGDClassifier(loss = 'log_loss').fit(X_train_new, Y_train)\n",
    "    \n",
    "    X_dev_new = feature_selector.transform(X_vec_dev)\n",
    "    Y_dev_pred = log_reg_clf.predict(X_dev_new)\n",
    "    accuracy = accuracy_score(Y_dev, Y_dev_pred)\n",
    "\n",
    "    if accuracy > accuracy_score_bst:\n",
    "        log_reg_clf_bst = log_reg_clf\n",
    "        accuracy_score_bst = accuracy\n",
    "        feature_selector_bst = feature_selector\n",
    "        num_features_bst = num_features\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "194141da-6ac9-4516-b159-b6b9fabe491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94        53\n",
      "           1       1.00      1.00      1.00        46\n",
      "           2       0.97      0.93      0.95        40\n",
      "           3       0.89      1.00      0.94        40\n",
      "           4       0.98      0.95      0.97        44\n",
      "\n",
      "    accuracy                           0.96       223\n",
      "   macro avg       0.96      0.96      0.96       223\n",
      "weighted avg       0.96      0.96      0.96       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use the tuned model to test\n",
    "X_test_new = feature_selector_bst.transform(X_vec_test)\n",
    "\n",
    "Y_test_pred = log_reg_clf_bst.predict(X_test_new)\n",
    "\n",
    "\n",
    "print(num_features_bst)\n",
    "\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311_venv",
   "language": "python",
   "name": "python311_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
